{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"Reference_Notebook_Milestone_2_Classification.ipynb","provenance":[],"collapsed_sections":["jSUhpu0fuS-M","La_3-i5quS-P","eTdaiYeMuS-a","RZj-by40uS-c","4ouIrxTDuS-f","e1WM0T3vuS-g","k7X4-6d-uS-i","LcsyeOmMuS-n","aO6ZhcAGuS-o","Z_npczaCuS-p","n85UqWwJuS-q","hD_3cgYvuS-r","7GtaZ2_juS-s","kjPqLicruS-s","LWj_L0VnuS-s","5Nd9-d44uS-t","0e6nxW5zuS-t","YgPleHWHuS-u","DXlpWpwquS-u","4IG8BCgluS-u","9MGpWZ8buS-v","DaH64bDMuS-v","HCBcKpE3uS-w","JJP3x99guS-w","sKMujUrwuS-w","HU_b2tY3uS-x","nFRFdf0KuS-x","2Yju5_bLuS-y","Mawgr8A6uS-z","oepvVX5CuS-3","Z02xcdkcuS-5"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"PHJaS1l-uS-B"},"source":["# **Milestone 2**"]},{"cell_type":"markdown","metadata":{"id":"Cdqhtr8yuS-L"},"source":["## **Model Building - Approach**\n","1. Data preparation\n","2. Partition the data into train and test set\n","3. Fit on the train data\n","4. Tune the model and prune the tree, if required\n","5. Test the model on test set"]},{"cell_type":"markdown","metadata":{"id":"7QgAXSWfuS-M"},"source":["## **Data Preparation**"]},{"cell_type":"markdown","metadata":{"id":"jSUhpu0fuS-M"},"source":["### **Separating the target variable from other variables**"]},{"cell_type":"code","metadata":{"id":"2_IZEwKruS-N"},"source":["# Drop the dependent variable from the dataframe and create the X(independent variable) matrix\n","# Remove _________ and complete the code\n","X = ____________________\n","\n","# Create dummy variables for the categorical variables - Hint: use the get_dummies() function\n","# Remove _________ and complete the code\n","X = ______________________\n","\n","# Create y(dependent varibale)\n","# Remove _________ and complete the code\n","\n","y = ___________________________-"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"La_3-i5quS-P"},"source":["### **Splitting the data into 70% train and 30% test set**"]},{"cell_type":"code","metadata":{"id":"sgF5mdtCuS-P"},"source":["# Split the data into training and test set\n","# Remove _________ and complete the code\n","\n","\n","_____________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"181uwUApuS-R"},"source":["### **Think about it** \n","- You can try different splits like 70:30 or 80:20 as per your choice. Does this change in split affect the performance?\n","- If the data is imbalanced, can you make the split more balanced and if yes, how?"]},{"cell_type":"markdown","metadata":{"id":"EVxWdZc_uS-S"},"source":["## **Model Evaluation Criterion**\n","\n","#### After understanding the problem statement, think about which evaluation metrics to consider and why. "]},{"cell_type":"code","metadata":{"id":"oNVO2cX_uS-V"},"source":["#creating metric function \n","def metrics_score(actual, predicted):\n","    print(classification_report(actual, predicted))\n","    cm = confusion_matrix(actual, predicted)\n","    plt.figure(figsize=(8,5))\n","    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels=['Not Eligible', 'Eligible'], yticklabels=['Not Eligible', 'Eligible'])\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTdaiYeMuS-a"},"source":["### **Build a Logistic Regression Model** "]},{"cell_type":"code","metadata":{"id":"OkEsDyg-uS-b"},"source":["# Defining the Logistic regression model\n","# Remove _________ and complete the code\n","____________\n","\n","# Fitting the model on the training data \n","# Remove _________ and complete the code\n","\n","________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZj-by40uS-c"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"id":"ga0ZoViNuS-d"},"source":["#Predict for train set\n","# Remove _________ and complete the code\n","________________\n","\n","#checking the performance on the train dataset\n","# Remove _________ and complete the code\n","_________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ouIrxTDuS-f"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"YtQ3PUoAuS-f"},"source":["#Predict for test set\n","# Remove _________ and complete the code\n","\n","_______________________\n","\n","#checking the performance on the test dataset\n","# Remove _________ and complete the code\n","\n","_____________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0Uix_PRuS-g"},"source":["**Observations: __________**"]},{"cell_type":"markdown","metadata":{"id":"e1WM0T3vuS-g"},"source":["#### Let's check the coefficients, and check which variables are important and how they affect the process of loan approval"]},{"cell_type":"code","metadata":{"id":"Uelo699muS-h"},"source":["# Printing the coefficients of logistic regression\n","# Remove _________ and complete the code\n","\n","\n","_____________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQAjCv4MuS-i"},"source":["**Insights ________**"]},{"cell_type":"markdown","metadata":{"id":"k7X4-6d-uS-i"},"source":["### **Think about it:**\n","- The above Logistic regression model was build on the threshold of 0.5, can we use different threshold?\n","- How to get an optimal threshold and which curve will help you achieve?\n","- How does, accuracy, precision and recall change on the threshold?"]},{"cell_type":"markdown","metadata":{"id":"Qn8IjxI7uS-j"},"source":["### **Build a Decision Tree Model**"]},{"cell_type":"markdown","metadata":{"id":"ZOYH3hhauS-l"},"source":["### **Think about it:**\n","- In Logistic regression we treated the outliers and built the model, should we do the same for tree based models or not? If not, why?"]},{"cell_type":"markdown","metadata":{"id":"LcsyeOmMuS-n"},"source":["#### Data Preparation for the tree based model"]},{"cell_type":"code","metadata":{"id":"syncXvl-uS-n"},"source":["# Add binary flags\n","# List of columns that has missing values in it\n","missing_col = [col for col in data.columns if data[col].isnull().any()]\n","\n","for colmn in missing_col:\n","    add_binary_flag(data,colmn)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUaxm207uS-o"},"source":["#  Treat Missing values in numerical columns with median and mode in categorical variables\n","# Select numeric columns.\n","num_data = data.select_dtypes('number')\n","\n","# Select string and object columns.\n","cat_data = data.select_dtypes('category').columns.tolist()#df.select_dtypes('object')\n","\n","# Fill numeric columns with median.\n","# Remove _________ and complete the code\n","data[num_data.columns] = num_data._________________\n","\n","# Fill object columns with model.\n","# Remove _________ and complete the code\n","for column in cat_data:\n","    mode = data[column].mode()[0]\n","    data[column] = data[column].____________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aO6ZhcAGuS-o"},"source":["#### Separating the target variable y and independent variable x"]},{"cell_type":"code","metadata":{"id":"t63yXVu_uS-p"},"source":["# Drop dependent variable from dataframe and create the X(independent variable) matrix\n","# Remove _________ and complete the code\n","\n","X = ____________________\n","\n","# Create dummy variables for the categorical variables - Hint: use the get_dummies() function\n","# Remove _________ and complete the code\n","X = ______________________\n","\n","# Create y(dependent varibale)\n","# Remove _________ and complete the code\n","\n","y = ___________________________-"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_npczaCuS-p"},"source":["#### Split the data"]},{"cell_type":"code","metadata":{"id":"5AEW52XOuS-p"},"source":["# Split the data into training and test set\n","# Remove _________ and complete the code\n","\n","\n","______________ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_n6jIx9yuS-q"},"source":["#Defining Decision tree model with class weights class_weight={0: 0.2, 1: 0.8}\n","# Remove ___________ and complete the code\n","\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BongVVR0uS-q"},"source":["#fitting Decision tree model\n","# Remove ___________ and complete the code\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n85UqWwJuS-q"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"id":"z2noEOCLuS-r"},"source":["# Checking performance on the training data\n","# Remove ___________ and complete the code\n","\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hD_3cgYvuS-r"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"ajlKdERCuS-r"},"source":["# Checking performance on the testing data\n","# Remove _________ and complete the code\n","\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUDAFw4cuS-r"},"source":["**Insights _____________**"]},{"cell_type":"markdown","metadata":{"id":"7GtaZ2_juS-s"},"source":["### **Think about it:**\n","- Can we improve this model? \n","- How to get optimal parameters in order to get the best possible results?"]},{"cell_type":"markdown","metadata":{"id":"kjPqLicruS-s"},"source":["### **Decision Tree - Hyperparameter Tuning**\n","\n","* Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.\n","* **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.** \n","* **It is an exhaustive search** that is performed on the specific parameter values of a model.\n","* The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.\n","\n","**Criterion {“gini”, “entropy”}**\n","\n","The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n","\n","**max_depth** \n","\n","The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n","\n","**min_samples_leaf**\n","\n","The minimum number of samples is required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n","\n","You can learn about more Hyperpapameters on this link and try to tune them. \n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"]},{"cell_type":"markdown","metadata":{"id":"LWj_L0VnuS-s"},"source":["#### Using GridSearchCV for Hyperparameter tuning on the model"]},{"cell_type":"code","metadata":{"id":"_v-aQT3SuS-s"},"source":["# Choose the type of classifier. \n","# Remove _________ and complete the code\n","________________\n","\n","\n","# Grid of parameters to choose from\n","# Remove _________ and complete the code\n","________________\n","\n","\n","# Type of scoring used to compare parameter combinations\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","# Run the grid search\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","# Fit the GridSearch on train dataset\n","# Remove _________ and complete the code\n","__________________\n","\n","\n","# Set the clf to the best combination of parameters\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","# Fit the best algorithm to the data. \n","# Remove _________ and complete the code\n","_________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Nd9-d44uS-t"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"id":"RJuFkZwluS-t"},"source":["# Checking performance on the training data based on the tuned model\n","# Remove _________ and complete the code\n","\n","______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0e6nxW5zuS-t"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"PXhwj5UtuS-t"},"source":["# Checking performance on the testing data based on the tuned model\n","# Remove _________ and complete the code\n","\n","_________________\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sskSDE2RuS-u"},"source":["**Insights ___________**"]},{"cell_type":"markdown","metadata":{"id":"YgPleHWHuS-u"},"source":["#### Plotting the Decision Tree"]},{"cell_type":"code","metadata":{"id":"W9alMIpUuS-u"},"source":["# Plot the decision  tree and analyze it to build the decision rule\n","# Remove _________ and complete the code\n","\n","\n","____________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXlpWpwquS-u"},"source":["#### Deduce the business rules apparent from the Decision Tree and write them down: _____"]},{"cell_type":"markdown","metadata":{"id":"4IG8BCgluS-u"},"source":["### **Building a Random Forest Classifier**\n","\n","**Random Forest is a bagging algorithm where the base models are Decision Trees.** Samples are taken from the training data and on each sample a decision tree makes a prediction. \n","\n","**The results from all the decision trees are combined together and the final prediction is made using voting or averaging.**"]},{"cell_type":"code","metadata":{"id":"2CS_aJmluS-u"},"source":["# Defining Random forest CLassifier\n","# Remove _________ and complete the code\n","\n","___________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9MGpWZ8buS-v"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"id":"E8CrEYjsuS-v"},"source":["#Checking performance on the training data\n","# Remove _________ and complete the code\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DaH64bDMuS-v"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"At9Nu5KkuS-v"},"source":["# Checking performance on the test data\n","# Remove _________ and complete the code\n","\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqQfEbEKuS-v"},"source":["**Observations: __________**"]},{"cell_type":"markdown","metadata":{"id":"HCBcKpE3uS-w"},"source":["### **Build a Random Forest model with Class Weights**"]},{"cell_type":"code","metadata":{"id":"bRsYlxyXuS-w"},"source":["# Defining Random Forest model with class weights class_weight={0: 0.2, 1: 0.8}\n","\n","# Remove _________ and complete the code\n","\n","_____________________________\n","\n","# Fitting Random Forest model\n","# Remove _________ and complete the code\n","\n","_______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJP3x99guS-w"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Xg6LNpXPuS-w"},"source":["# Checking performance on the train data\n","# Remove _________ and complete the code\n","\n","________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKMujUrwuS-w"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"ehxp227duS-x"},"source":["# Checking performance on the test data\n","# Remove _________ and complete the code\n","\n","________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HU_b2tY3uS-x"},"source":["### **Think about it:**\n","- Can we try different weights?\n","- If yes, should we increase or decrease class weights for different classes? "]},{"cell_type":"markdown","metadata":{"id":"nFRFdf0KuS-x"},"source":["### **Tuning the Random Forest**"]},{"cell_type":"markdown","metadata":{"id":"-pHpGmcquS-x"},"source":["* Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.\n","* **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.** \n","* **It is an exhaustive search** that is performed on the specific parameter values of a model.\n","* The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.\n","\n","\n","**n_estimators**: The number of trees in the forest.\n","\n","**min_samples_split**: The minimum number of samples required to split an internal node:\n","\n","**min_samples_leaf**: The minimum number of samples required to be at a leaf node. \n","\n","**max_features{“auto”, “sqrt”, “log2”, 'None'}**: The number of features to consider when looking for the best split.\n","\n","- If “auto”, then max_features=sqrt(n_features).\n","\n","- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n","\n","- If “log2”, then max_features=log2(n_features).\n","\n","- If None, then max_features=n_features.\n","\n","You can learn more about Random Forest Hyperparameters from the link given below and try to tune them\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"]},{"cell_type":"markdown","metadata":{"id":"2Yju5_bLuS-y"},"source":["#### **Warning:** This may take a long time depending on the parameters you tune. "]},{"cell_type":"code","metadata":{"id":"4eF3pM_0uS-y"},"source":["# Choose the type of classifier. \n","# Remove _________ and complete the code\n","________________\n","\n","\n","# Grid of parameters to choose from\n","# Remove _________ and complete the code\n","________________\n","\n","\n","# Type of scoring used to compare parameter combinations\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","# Run the grid search\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","#fit the GridSearch on train dataset\n","# Remove _________ and complete the code\n","__________________\n","\n","\n","# Set the clf to the best combination of parameters\n","# Remove _________ and complete the code\n","_________________\n","\n","\n","# Fit the best algorithm to the data. \n","# Remove _________ and complete the code\n","_________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mawgr8A6uS-z"},"source":["#### Checking the performance on the train dataset"]},{"cell_type":"code","metadata":{"id":"gm8MPNCOuS-z"},"source":["# Checking performance on the training data\n","# Remove _________ and complete the code\n","______________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oepvVX5CuS-3"},"source":["#### Checking the performance on the test dataset"]},{"cell_type":"code","metadata":{"id":"1PoLfAzauS-3"},"source":["# Checking performace on test dataset\n","# Remove _________ and complete the code\n","\n","_________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bhHvKnAuS-4"},"source":["**Insights: _____**"]},{"cell_type":"markdown","metadata":{"id":"Pq9GKJxnuS-4"},"source":["#### Plot the Feature importance of the tuned Random Forest"]},{"cell_type":"code","metadata":{"id":"PeJMlclxuS-4"},"source":["# importance of features in the tree building ( The importance of a feature is computed as the \n","#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n","# Checking performace on test dataset\n","# Remove _________ and complete the code\n","\n","_________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z02xcdkcuS-5"},"source":["### **Think about it:**\n","- We have only built 3 models so far, Logistic Regression, Decision Tree and Random Forest \n","- We can build other Machine Learning classification models like kNN, LDA, QDA or even Support Vector Machines (SVM).\n","- Can we also perform feature engineering and create model features and build a more robust and accurate model for this problem statement? "]},{"cell_type":"markdown","metadata":{"id":"fkR0P8HXuS-5"},"source":["### **Comparing Model Performances**"]},{"cell_type":"code","metadata":{"id":"pe6Pl8FVuS-6"},"source":["def get_recall_score(model,flag=True,X_train=X_train,X_test=X_test):\n","    '''\n","    model : classifier to predict values of X\n","\n","    '''\n","    a = [] # defining an empty list to store train and test results\n","    pred_train = model.predict(X_train)\n","    pred_test = model.predict(X_test)\n","    train_recall = metrics.recall_score(y_train,pred_train)\n","    test_recall = metrics.recall_score(y_test,pred_test)\n","    a.append(train_recall) # adding train recall to list \n","    a.append(test_recall) # adding test recall to list\n","    \n","    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n","    if flag == True: \n","        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n","        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n","    \n","    return a # returning the list with train and test scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHrSzelxuS-6"},"source":["##  Function to calculate precision score\n","def get_precision_score(model,flag=True,X_train=X_train,X_test=X_test):\n","    '''\n","    model : classifier to predict values of X\n","\n","    '''\n","    b = []  # defining an empty list to store train and test results\n","    pred_train = model.predict(X_train)\n","    pred_test = model.predict(X_test)\n","    train_precision = metrics.precision_score(y_train,pred_train)\n","    test_precision = metrics.precision_score(y_test,pred_test)\n","    b.append(train_precision) # adding train precision to list\n","    b.append(test_precision) # adding test precision to list\n","    \n","    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n","    if flag == True: \n","        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n","        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n","\n","    return b # returning the list with train and test scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UbeXu-guS-7"},"source":["##  Function to calculate accuracy score\n","def get_accuracy_score(model,flag=True,X_train=X_train,X_test=X_test):\n","    '''\n","    model : classifier to predict values of X\n","\n","    '''\n","    c = [] # defining an empty list to store train and test results\n","    train_acc = model.score(X_train,y_train)\n","    test_acc = model.score(X_test,y_test)\n","    c.append(train_acc) # adding train accuracy to list\n","    c.append(test_acc) # adding test accuracy to list\n","    \n","    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n","    if flag == True:\n","        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n","        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n","    \n","    return c # returning the list with train and test scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZ9-xSnjuS-8"},"source":["# Make the list of all the model names \n","\n","models = [___________________________]\n","# Remove _________ and complete the code\n","\n","# defining empty lists to add train and test results\n","acc_train = []\n","acc_test = []\n","recall_train = []\n","recall_test = []\n","precision_train = []\n","precision_test = []\n","\n","# looping through all the models to get the accuracy,recall and precision scores\n","for model in models:\n","     # accuracy score\n","    j = get_accuracy_score(model,False)\n","    acc_train.append(j[0])\n","    acc_test.append(j[1])\n","\n","    # recall score\n","    k = get_recall_score(model,False)\n","    recall_train.append(k[0])\n","    recall_test.append(k[1])\n","\n","    # precision score\n","    l = get_precision_score(model,False)\n","    precision_train.append(l[0])\n","    precision_test.append(l[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHsbc4DMuS-8"},"source":["# Mention the Model names in the list. for example 'Model': ['Decision Tree', 'Tuned Decision Tree'..... write tht names of all model built]\n","# Remove _________ and complete the code\n","\n","comparison_frame = pd.DataFrame({'Model':[______________________], \n","                                          'Train_Accuracy': acc_train,\n","                                          'Test_Accuracy': acc_test,\n","                                          'Train_Recall': recall_train,\n","                                          'Test_Recall': recall_test,\n","                                          'Train_Precision': precision_train,\n","                                          'Test_Precision': precision_test}) \n","comparison_frame"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxFPX-pduS-9"},"source":["**Insights: ________**"]},{"cell_type":"markdown","metadata":{"id":"kPywjJo6uS-9"},"source":["**1. Refined insights -** What are the most meaningful insights from the data relevant to the problem?"]},{"cell_type":"markdown","metadata":{"id":"eZdLjL5vuS-9"},"source":["**2. Comparison of various techniques and their relative performance -** How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?"]},{"cell_type":"markdown","metadata":{"id":"lJRsBfsruS--"},"source":["**3. Proposal for the final solution design -** What model do you propose to be adopted? Why is this the best solution to adopt?"]}]}